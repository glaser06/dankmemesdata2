{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "27ce94da-0235-4af5-b2f7-5b6509511f3c"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Twitch.tv is a platform where users can stream videos of themselves playing video games live. One of the prominent features of twitch.tv is that each channel has a unique IRC channel that allows users who are viewing that channel to communicate. Due to the anonymity of the users in the chat though, civil discussion rarely takes place. More often than not, and especially in channels with large amounts of viewers, the chat devolves into a large portion of the users all concurrently spamming one or two different messages. The picture below is an example of this:\n",
    "\n",
    "![Image of Chat](https://i.gyazo.com/554f05d25f4c18d5dd8e64a4cd009840.png)\n",
    "\n",
    "In addition to text, twitch has a large quantity of humorous built-in emoticons that feature prominent streamers faces. Because of this, the chat often devolves into all of the users spamming the same face. For the sake of this project, we will refer to messages that are commonly spammed as memes.\n",
    "\n",
    "With this, we pose the question: How do the content and virality of memes affect the overall composition of twitch chat rooms and the advent of new and future memes?\n",
    "\n",
    "We have sampled twitch chat data from 10 popular League of Legends streamers, all of which pull in on average 20,000+ viewers per streaming session. While these viewers may not be concurrent nor even unique per stream, we attempt to assess the content of the memes to see whether or not individual streaming personalities can create their own community memes that don't carry over from channel. We first see if we can classify streams by their twitch chat content. We will run a multinomial Naive bayes classifier on text data from different streams with the documents being different stream sessions. We then will attempt to generate memes with an ngram model to see if these memes are different in scale/content.\n",
    "\n",
    "### Getting Data\n",
    "\n",
    "The first task is to pull data from a twitch chat that's currently active. Twitch does not store chat logs for the average user to see, so we need to use a bot to sit in streamers chat rooms and pull all incoming messages and store them to a file.\n",
    "\n",
    "Our group didn't have much experience mining IRC chats with text and sockets, so we found a twitch chat bot online and modified it for our needs. The link to the github we used is in the code block.\n",
    "\n",
    "We made an account 'dankmemebot1' and generated an oauth key for that account using Twitch's API. We then registered a list of channels we would pull data from and set up files to store the extracted text in. The account was used to enter the IRC of each channel and to send messages later on to see whether our ngram generated memes would pass as user generated memes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "40be4587-f94e-4dcf-afd2-1080306f0a50"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up for methods and stuff\n",
    "#https://github.com/RubbixCube/Twitch-Chat-Bot-V2\n",
    "\n",
    "import datetime\n",
    "import socket\n",
    "import select\n",
    "import re\n",
    "\n",
    "channels = [\n",
    "    'tsm_bjergsen',\n",
    "    'c9sneaky',\n",
    "    'trick2g',\n",
    "    'tsm_doublelift',\n",
    "    'nightblue3',\n",
    "    'imaqtpie',\n",
    "    'rush',\n",
    "    'admiralbulldog',\n",
    "    'tsm_theoddone',\n",
    "    'wingsofdeath'\n",
    "    ]\n",
    "username = 'dankmemebot1'\n",
    "oauth = 'oauth:f1d7mm17vlzjols100etso2zg9jqru'\n",
    "\n",
    "channelfiles = {}\n",
    "\n",
    "for name in channels:\n",
    "    channelfiles[name] = open(name+'.txt', 'a+')\n",
    "#     channelfiles[name].write(\"\\n \\n------------ \\n \\n \\n \\n\")\n",
    "#     channelfiles[name].write(\"new session\\n\")\n",
    "#     channelfiles[name].write(\"\\n \\n \\n ------------ \\n \\n \\n\")\n",
    "\n",
    "def ping():\n",
    "    ''' Respond to the server 'pinging' (Stays connected) '''\n",
    "    socks[0].send('PONG :pingis\\n')\n",
    "    print('PONG: Client > tmi.twitch.tv')\n",
    "\n",
    "def sendmsg(chan,msg):\n",
    "    ''' Send specified message to the channel '''\n",
    "    socks[0].send('PRIVMSG '+chan+' :'+msg+'\\n')\n",
    "    print('[BOT] -> '+chan+': '+msg+'\\n')\n",
    "\n",
    "def sendwhis(user,msg):\n",
    "    socks[1].send('PRIVMSG #jtv :/w '+user+' '+msg+'\\n')\n",
    "    print('[BOT] -> '+user+': '+msg+'\\n')\n",
    "\n",
    "def getmsg(msg):\n",
    "    ''' GET IMPORTANT MESSAGE '''\n",
    "    if(re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)):\n",
    "        msg_edit = msg.split(':',2)\n",
    "        if(len(msg_edit) > 2):\n",
    "            user = msg_edit[1].split('!',1)[0] # User\n",
    "            message = msg_edit[2] # Message\n",
    "            channel = re.findall('PRIVMSG (.*)',msg_edit[1]) # Channel\n",
    "#             print channel[0][1:-1]\n",
    "            privmsg = re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)\n",
    "            ''' CONVERT TO ARRAY '''\n",
    "            privmsg = [x for xs in privmsg for x in xs]\n",
    "\n",
    "            datelog = datetime.datetime.now()\n",
    "\n",
    "            ''' PRINT TO CONSOLE '''\n",
    "            if(len(privmsg) > 0):\n",
    "                if len(channel) > 0:\n",
    "                    chan = channel[0] \n",
    "                    if len(chan) >= 3:\n",
    "    \n",
    "                        #print ('['+str(datelog.hour)+':'+str(datelog.minute)+':'\n",
    "#         +str(datelog.second)+'] '+user+' @ '+channel[0][:-1]+': '+message+'\\n')\n",
    "                        channelfiles[channel[0][1:-1]].write('['+str(datelog.day)+':'\n",
    "                                                             +str(datelog.hour)+':'+str(datelog.minute)+':'\n",
    "                                                             +str(datelog.second)+'] '+user+' @ '+channel[0][:-1]\n",
    "                                                             +': '+message+'\\n')\n",
    "\n",
    "print \"finished\"\n",
    "socks = [socket.socket(),socket.socket()]\n",
    "socks[0].connect(('irc.twitch.tv',6667))\n",
    "\n",
    "socks[0].send('PASS '+oauth+'\\n')\n",
    "socks[0].send('NICK '+username+'\\n')\n",
    "\n",
    "#https://github.com/RubbixCube/Twitch-Chat-Bot-V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we had to use these methods and the API to save the chat from when we monitored it. Again, we didn't really know how to do this, but the github had an example way of connecting to a server, so we followed that and read chat from all of the registered list of channels and saved them all. We saved the data from each channel in its own text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/RubbixCube/Twitch-Chat-Bot-V2\n",
    "\n",
    "socks = [socket.socket(),socket.socket()]\n",
    "\n",
    "socks[0].connect(('irc.twitch.tv',6667))\n",
    "\n",
    "socks[0].send('PASS '+oauth+'\\n')\n",
    "socks[0].send('NICK '+username+'\\n')\n",
    "\n",
    "for val in channels:\n",
    "\n",
    "    socks[0].send('JOIN #'+val+'\\n')\n",
    "    \n",
    "print('Connected to irc.twitch.tv on port 6667')\n",
    "print('USER: '+username)\n",
    "print('OAUTH: oauth:'+'*'*len(oauth))\n",
    "print('\\n')\n",
    "\n",
    "temp = 0\n",
    "count = 0\n",
    "while True:\n",
    "  \n",
    "    (sread,swrite,sexc) = select.select(socks,socks,[],120)\n",
    "    for sock in sread:\n",
    "  \n",
    "        ''' Receive data from the server '''\n",
    "        msg = sock.recv(2048)\n",
    "        if(msg == ''):\n",
    "            temp + 1\n",
    "            if(temp > 5):\n",
    "                print('Connection might have been terminated')\n",
    "    \n",
    "        ''' Remove any linebreaks from the message '''\n",
    "        msg = msg.strip('\\n\\r')\n",
    "\n",
    "        ''' DISPLAY MESSAGE IN SHELL '''\n",
    "        getmsg(msg)\n",
    "#         print(msg)\n",
    "\n",
    "        # ANYTHING TO DO WITH CHAT FROM CHANNELS\n",
    "        ''' GET THE INFO FROM THE SERVER '''\n",
    "        check = re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)\n",
    "        if(len(check) > 0):\n",
    "            msg_edit = msg.split(':',2)\n",
    "            if(len(msg_edit) > 2):\n",
    "                user = msg_edit[1].split('!',1)[0] # User\n",
    "                message = msg_edit[2] # Message\n",
    "                channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "                msg_split = str.split(message)\n",
    "                        \n",
    "        # ANYTHING TO DO WITH WHISPERS RECIEVED FROM USERS\n",
    "        check = re.findall('@(.*).tmi.twitch.tv WHISPER (.*) :(.*)',msg)\n",
    "        if(len(check) > 0):\n",
    "            msg_edit = msg.split(':',2)\n",
    "            if(len(msg) > 2):\n",
    "                user = msg_edit[1].split('!',1)[0] # User\n",
    "                message = msg_edit[2] # Message\n",
    "                channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "\n",
    "                whis_split = str.split(message)                           \n",
    "\n",
    "        ''' Respond to server pings '''\n",
    "        if msg.find('PING :') != -1:\n",
    "            print('PING: tmi.twitch.tv > Client')\n",
    "            ping()\n",
    "            \n",
    "print \"stopped\"\n",
    "\n",
    "#https://github.com/RubbixCube/Twitch-Chat-Bot-V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c6825f66-5cc4-443e-93bf-f36d69fb20b9"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Now we needed to parse the raw output from the IRC into just the messages themselves because we don't care about who was sending the message or the timestamp. An example message output from the IRC looks like:\n",
    "\n",
    "[11:12:22:40] delriopie @ #rush: PogChamp\n",
    "\n",
    "So this really is just a matter of stripping all of the text before the colon and storing it somewhere. We just put each specific channel's parsed text into another text file for further storage. All of this was just simple string processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7eed4446-dd94-4b06-bd61-cab103f3a29c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONGER gachiGASM \r\n",
      "\n",
      "SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS Sou\\rPls qtpDANCE qtpPLS SourPls qtpDANCE . \r\n",
      "\n",
      "qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls \r\n",
      "\n",
      "DOO DOO DOO DOOO bunWeeb \r\n",
      "\n",
      "YES \r\n",
      "\n",
      "UNSUBBED \r\n",
      "\n",
      "TT \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parseTwitchChat(textpath):\n",
    "    #returns an array of text\n",
    "    text = []\n",
    "    with open(textpath, 'r') as f:\n",
    "        for line in f:\n",
    "            text.append(line.split())\n",
    "    count = 0\n",
    "    newtext = []\n",
    "    for line in text:\n",
    "        \n",
    "        if len(line) > 2:\n",
    "            count += 1\n",
    "            if line[1] == 'PRIVMSG':\n",
    "                line = line[3:]\n",
    "                \n",
    "                line[0] = line[0][1:]\n",
    "                newtext.append(line)\n",
    "#                 if count <= 10:\n",
    "#                     print line\n",
    "            else:    \n",
    "                line = line[4:]\n",
    "                newtext.append(line)\n",
    "#                 if count <= 10:\n",
    "#                     print line\n",
    "\n",
    "\n",
    "    return newtext\n",
    "def parseAllFiles():\n",
    "    \n",
    "    textFiles = {}\n",
    "    labels = []\n",
    "\n",
    "    for i in channels:\n",
    "        textFiles[i] = (parseTwitchChat(i + \".txt\"))\n",
    "        labels.append(i)\n",
    "\n",
    "    for i in channels:\n",
    "\n",
    "        for k in range(len(textFiles[i])): #for array inside textfile\n",
    "            if (k % 1000) == 0:\n",
    "                with open('league/'+i+'/' + i + '_' + str(k) +'_parsed.txt', 'a+') as f: #open every 1000 lines\n",
    "                    f.truncate()\n",
    "                    lenleftover = len(textFiles[i]) - k\n",
    "                    if lenleftover >= 1000:\n",
    "                        lenleftover = 1000\n",
    "\n",
    "                    for j in range(lenleftover): #indivudal word in array\n",
    "                        for l in textFiles[i][j+k]: #iterate through 1000 times for every 1000 lines\n",
    "                            f.write(l +' ') #l = word\n",
    "                        f.write('\\n')\n",
    "with open('league/c9sneaky/c9sneaky_0_parsed.txt','r') as f:\n",
    "    for i,b in enumerate(f):\n",
    "        print b\n",
    "        if i > 5: \n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "40a49f59-178b-4bd8-adba-2560cc5726c8"
    }
   },
   "source": [
    "## Meme Classification\n",
    "\n",
    "The first thing we want to check is whether we can classify streams by their twitch chat content. We run a multinomial Naive Bayes classifier on text data from different streams with the documents being different stream sessions. We take the different streams (by channel) as the labels that we input into our classifier. We then will take more twitch chat data and attempt to predict which stream it came from based off its content. Note that this task is not easy, often two chatrooms can look very similar despite being different channels. Below is an example of two completely different chatroom that look identical:\n",
    "\n",
    "![Image of Comparison](http://i.imgur.com/RLgwivV.png)\n",
    " \n",
    "We believe that Naive Bayes is an appropriate test for our question because we believe that despite the massive quantity of shared memes, different streams also have a fair amount of unique memes. The uniqueness of memes (which can often makeup large portions of chat) is in part due to the composition of the twitch chat rooms which are different per streamer. Many streamers have unique emotes for their own chatrooms too, making classifying messages that contain these emotes fairly easy (although often emotes from larger streamers spill over into the chat of other streamers). If we have a high success rate, we can conclude that memes do correlate to the composition of twitch chat rooms. However, if our classifier cannot correctly classify twitch chat data based on streamers, we assess that memes are not unique and are in fact shared between different streamers. We then conclude that twitch chat is homogenous in nature and is highly similar between streams. Although Naive Bayes is a simple classifier, it is very accurate and hence can cause some issues in our analysis. If our test documents are very similar, our classifier might classify it as different based off small intricacies between each stream. Although at this point it is hard to figure out what the minor differences are between streams, in general we must account for this concern in accuracy.\n",
    " \n",
    "We use sci-kits sklearn API with multinomialNB as our classifier. We then use other scikit tools such as count vectorizer and tfdif, in order to transform our data into machine learning recognizable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "125062b5-fbf2-4f06-9e58-331f0c0f2b62"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: 10\n",
      "total: 27\n",
      "error rate: 0.37037037037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import load_files\n",
    "# sources: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "#         http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#load files from very specific format check \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn-datasets-load-files\n",
    "files_train = load_files('league')\n",
    "\n",
    "#vectorize the data\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#vector transform \n",
    "# print unicode(files_train.data)\n",
    "X_train_counts = count_vect.fit_transform([unicode(s, errors='replace') for s in files_train.data])\n",
    "\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "\n",
    "X_train_tfidf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, files_train.target)\n",
    "\n",
    "files_test = load_files('test')\n",
    "\n",
    "test_vect = CountVectorizer()\n",
    "\n",
    "test_train_counts = count_vect.transform(files_test.data)\n",
    "\n",
    "Test_train_tfidf = tf_transformer.transform(test_train_counts)\n",
    "\n",
    "ypred = clf.predict(Test_train_tfidf)\n",
    "print \"errors: \" + str((files_test.target != ypred).sum())\n",
    "print \"total: \" + str(len(files_test.target))\n",
    "print \"error rate: \" +  str((files_test.target != ypred).sum()/(float(len(ypred))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dba1bdb2-8b74-4a79-b451-00636a89d79b"
    }
   },
   "source": [
    "This corresponds to an error rate of 10/27, which is approximately 37%. That's extremely good considering how often chat's look identical and how many different streams we were trying to differentiate between. If we were just blindly guessing our error rate would be around 90%! With an error rate of 37% this would lead us to believe that our sample of twitch chat data can be differentiated between and thus the content of memes do affect the composition of twitch chat.\n",
    "\n",
    "Apart from this, we cannot conclude much more; we can hypothesize that the responses that streamers give to viewers generate the similar reactions. We could surmise that viewers are not locked to only one stream and view many streams concurrently and thus take their memes with them. The specificty of the game also can be factor in determining chat meme content. It could also be possible that memes of the lowest common denominator appeal to a wider audience which could overwhelm the more localized memes that are prevalent per streamer.\n",
    "\n",
    "If we were doing further analysis, we could probably group different chatrooms into different categories; many streamers have varying levels of moderation and tolerate different memes differently. For example a few of the chatrooms we datamined contained a lot offensive content, whereas other chatrooms were heavily moderated and did not have that content. Other chatrooms disallow lots of spam. If we classified based on this, we could probably group the chatrooms accurately and have most of our error be in between groups.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Meme Generation\n",
    "\n",
    "In order to analyze how the content of memes affect future memes, we need a way to use old memes to create new memes and determine if these new memes are viral. An ngram language model is a simple language model that fits this exact need. The ngram language model trains on past data and extracts the ngrams, word tuples of length n, from the text. This model captures the relationship between words that frequently occur together, which it uses to generate phrases. \n",
    "\n",
    "To make a ngram model, we need to first process the data. We added !END! tags to the end of every chat line to indicate the end of a message. We then tagged all tokens with less than 5 occurances in the file as UNKNOWN words, which we will deal with later. We create our ngram model with all of our collected data in order to create the most wholesome memes. To generate a phrase, we start with a random ngram and look at all words that followed it in our data. We then randomly choose a word from that list with the most occuring word more likely to be chosen. We keep generating until the maximum length has been reached or an !END! tag has been reached. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b6f88f72-4ac7-4f7d-a3e2-47cd5a339fcb"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('haHAA', '!UNKNOWN!', None) : ['haHAA']\n",
      "('!UNKNOWN!', 'ANELE', None) : ['ANELE', 'in', '!UNKNOWN!']\n",
      "('BEST', 'SONG', 'bunWeeb') : [None, 'BEST', 'BEST', None, 'BEST', 'BEST', 'BEST', None, 'BEST', 'BEST', 'BEST', 'BEST', None, 'BEST', 'BEST', 'BEST', None, 'BEST', 'BEST', None]\n",
      "('Kreygasm', None, 'TT') : ['Kreygasm']\n",
      "('gachiGASM', None, 'SourPls') : ['qtpDANCE']\n",
      "('cmonBruh', 'cmonBruh', 'cmonBruh') : ['cmonBruh', None]\n",
      "('cmonBruh', None, 'Kreygasm') : ['Kreygasm']\n",
      "Model Created\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import nltk\n",
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "from itertools import groupby as g\n",
    "\n",
    "def tokenize_by_word(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def most_common(L):\n",
    "    return max(g(sorted(L)), key=lambda(x, v):(len(list(v)),-L.index(x)))[0]\n",
    "class NgramModel:\n",
    "    \"\"\"\n",
    "\n",
    "    __init__ initializes a NgramModel. \n",
    "    training preprocessing: add !START! and !END! tokens to each line. \n",
    "    Parameters:\n",
    "    n                       the n used in the ngram model\n",
    "    train                   a string containing a training text\n",
    "    unknown_replace_limit   if a word in the training text appears less than this many times, replace it with 'UNKNOWNWORD'\n",
    "    \"\"\"\n",
    "    def __init__(self, n,tokens=[], train=\"\",filenames=\"\", UnknownCount=0):\n",
    "        self.filenames = filenames\n",
    "        self.UnknownCount = UnknownCount\n",
    "        if (n < 0):\n",
    "            raise Exception(\"N must be greater than or equal to zero to make an Ngram model.\")\n",
    "        if len(tokens) == 0:\n",
    "            self.tokens = self.tokenize_from_files()\n",
    "            self.tag_with_unknown()\n",
    "        else:\n",
    "            self.tokens = tokens\n",
    "            self.tag_with_unknown()\n",
    "\n",
    "        self.n = n\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def tag_with_unknown(self):\n",
    "        token_count = Counter(self.tokens)\n",
    "        def mark(token):\n",
    "            if token_count[token] < self.UnknownCount:\n",
    "                return \"!UNKNOWN!\"\n",
    "            else:\n",
    "                return token\n",
    "        tokens = [mark(token) for token in self.tokens ]\n",
    "        self.tokens = tokens\n",
    "    def tokenize_from_files(self):\n",
    "        file_tokens = []\n",
    "        for fname in glob.glob(self.filenames):\n",
    "            with open(fname, 'r') as f:\n",
    "\n",
    "                for i in f:\n",
    "                    tokens = i.split()\n",
    "                    tokens.append(None)\n",
    "                    file_tokens.extend(tokens)\n",
    "        return file_tokens\n",
    "    def build_model(self):\n",
    "    #     build a language model from ngrams\n",
    "        tokens = self.tokens\n",
    "        n = self.n\n",
    "        model = dict()\n",
    "        if len(tokens) < n:\n",
    "            return model\n",
    "        for i in range(len(tokens) - n):\n",
    "            gram = tuple(tokens[i:i+n])\n",
    "            next_tok = tokens[i+n]\n",
    "            if gram in model:\n",
    "                model[gram].append(next_tok)\n",
    "            else: \n",
    "                model[gram] = [next_tok]\n",
    "        final_gram = tuple(tokens[len(tokens)-n:])\n",
    "\n",
    "        return model\n",
    "    def generate(self, seed=None, max_iterations=50):\n",
    "        model = self.model\n",
    "\n",
    "        n = self.n\n",
    "        seed = random.choice(model.keys())\n",
    "        flag = True\n",
    "        while flag:\n",
    "            flag = False\n",
    "            for i in seed:\n",
    "                if i is None or i == \"!UNKNOWN!\":\n",
    "                    flag = True\n",
    "            if flag:\n",
    "                seed = random.choice(model.keys())\n",
    "\n",
    "        output = list(seed)\n",
    "        current = tuple(seed)\n",
    "        for i in range(max_iterations):\n",
    "            next_token = random.choice(model.keys())[0]\n",
    "            while next_token == \"!UNKNOWN!\":\n",
    "                next_token = random.choice(model.keys())[0]\n",
    "            if current in model:\n",
    "                possible_next_tokens = model[current]\n",
    "                if len(possible_next_tokens) > 1:\n",
    "                    next_token = random.choice(possible_next_tokens)\n",
    "                    while next_token == \"!UNKNOWN!\":\n",
    "                        possible_next_tokens.remove(next_token)\n",
    "                        try:\n",
    "                            next_token = random.choice(possible_next_tokens)\n",
    "                        except:\n",
    "                            next_token = random.choice(model.keys())[0]\n",
    "                            while next_token == \"!UNKNOWN!\":\n",
    "                                next_token = random.choice(model.keys())[0]\n",
    "\n",
    "            if next_token is None: break\n",
    "            output.append(next_token)\n",
    "            current = tuple(output[-n:])\n",
    "            \n",
    "        return output\n",
    "\n",
    "files = []\n",
    "file_tokens = []\n",
    "for fname in glob.glob(\"league/*/*.txt\"):\n",
    "    with open(fname, 'r') as f:\n",
    "        \n",
    "        for i in f:\n",
    "            tokens = i.split()\n",
    "            if len(tokens) > 0 and \"@\" not in tokens[0] and \"!\" not in tokens[0] :\n",
    "                tokens.append(None)\n",
    "                file_tokens.extend(tokens)\n",
    "\n",
    "    break\n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "bigram = NgramModel(3, file_tokens, filenames=\"league/*/*.txt\", UnknownCount=15)\n",
    "counter = 0\n",
    "\n",
    "for i in bigram.model:\n",
    "    print i,\":\", bigram.model[i]\n",
    "    if counter > 5:\n",
    "        break\n",
    "    counter += 1\n",
    "print \"Model Created\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just creates a model with which we can generate memes; to generate a list of memes we call the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "200cd011-a795-4736-8c68-03ab562ce129"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE\n",
      "\n",
      "Ban bunWeeb .\n",
      "\n",
      "bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb bunWeeb Me Weeb bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb\n",
      "\n",
      "VoHiYo PogChamp Kreygasm LOL If cmonBruh\n",
      "\n",
      "IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited gachiGASM bunWeeb LUL SneakyPls\n",
      "\n",
      "LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT Jebaited LINK IT\n",
      "\n",
      "ResidentSleeper ResidentSleeper ResidentSleeper TriHard haHAA PogChamp ANELE the qtpDANCE LOL qtpPLS ME\n",
      "\n",
      "TT VoHiYo PogChamp sneakyFedora Kappa ResidentSleeper cmonBruh sneakyFedora is LUL I\n",
      "\n",
      "LOL LOL LOL SwiftRage cmonBruh WutFace this PogChamp LUL HotPokket bunWeeb Me\n",
      "\n",
      "Ban bunWeeb bunWeeb Me Weeb bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fresh_memes = []\n",
    "for i in range(10):\n",
    "    meme = \" \".join(bigram.generate())\n",
    "    print meme\n",
    "    print \n",
    "    fresh_memes.append(meme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a majority of our generated messages contain emotes in them, like \"SourPls.\"  This is expected, as a majority of twitch message are just emotes with some loosely-related text inbetween. However, one of the problems that arises because of this is that often times our generated messages are just incomplete nonsense with random emotes as spacing. Another issue that arises is that the chatrooms with more spam has a significantly larger effect on the generated memes. As you can see a majority of the emotes start with the words 'Sneaky' or 'qtp', which means that they each are associated to the streamers Sneaky and QTPie. These streamers have notoriously low moderation and have a ridiculous amount of spam passing through them. For example, despite the fact that we pulled the chat from each stream for a similar amount of time and that a majority of streamers had similar amounts of views while we were recording their chats, Sneaky and QTPie each had over 5 thousand more messages than the next highest streamers. Clearly this is represented in our memes.\n",
    "\n",
    "Next we want to measure how our computer generated memes stack up to human generated memes. Typically a succesful human generated meme will gather traction and be spammed by other users after being entered into the chat a few times. So we wrote a script that does that. Typically twitch has the ability to detect a chatbot if it enters messages too fast, so we set up our script to spam the input message once ever 30 seconds for 5 minutes and store all of the resulting output in a text file. We then counted how many times it was spammed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls qtpDANCE SneakyPls\n",
      "Connected to irc.twitch.tv on port 6667\n",
      "USER: dankmemebot1\n",
      "OAUTH: oauth:******************************\n",
      "\n",
      "\n",
      "sneakyFedora sneakyFedora .\n",
      "Connected to irc.twitch.tv on port 6667\n",
      "USER: dankmemebot1\n",
      "OAUTH: oauth:******************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def spam(chan, fname, message1, username, oauth):\n",
    "\n",
    "    socks[0].send('JOIN '+chan+'\\n')\n",
    "        \n",
    "    print('Connected to irc.twitch.tv on port 6667')\n",
    "    print('USER: '+username)\n",
    "    print('OAUTH: oauth:'+'*'*30)\n",
    "    print('\\n')\n",
    "\n",
    "    temp = 0\n",
    "    count = 0\n",
    "    starttime = time.clock()\n",
    "    endtime = starttime\n",
    "    spamtime = -1\n",
    "\n",
    "    while (endtime-starttime) <= 180:\n",
    "      \n",
    "        (sread,swrite,sexc) = select.select(socks,socks,[],120)\n",
    "        for sock in sread:\n",
    "              \n",
    "            if spamtime == -1 or spamtime >= 30:\n",
    "                socks[0].send('PRIVMSG '+chan+' :'+message1+'\\n')\n",
    "#                 print \"spam\"+fname\n",
    "                spamtime = 0\n",
    "                \n",
    "            ''' Receive data from the server '''\n",
    "            msg = sock.recv(2048)\n",
    "            if(msg == ''):\n",
    "                temp + 1\n",
    "                if(temp > 5):\n",
    "                    print('Connection might have been terminated')\n",
    "        \n",
    "            ''' Remove any linebreaks from the message '''\n",
    "            msg = msg.strip('\\n\\r')\n",
    "\n",
    "            ''' DISPLAY MESSAGE IN SHELL '''\n",
    "            getmsg(msg)\n",
    "            with open(fname, 'a+') as f:\n",
    "                f.write(msg + ' \\n')\n",
    "                f.close()\n",
    "    #         print(msg)\n",
    "\n",
    "            # ANYTHING TO DO WITH CHAT FROM CHANNELS\n",
    "            ''' GET THE INFO FROM THE SERVER '''\n",
    "            check = re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)\n",
    "            if(len(check) > 0):\n",
    "                msg_edit = msg.split(':',2)\n",
    "                if(len(msg_edit) > 2):\n",
    "                    user = msg_edit[1].split('!',1)[0] # User\n",
    "                    message = msg_edit[2] # Message\n",
    "                    channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "\n",
    "                    msg_split = str.split(message)\n",
    "\n",
    "            ''' Respond to server pings '''\n",
    "            if msg.find('PING :') != -1:\n",
    "                print('PING: tmi.twitch.tv > Client')\n",
    "                ping()\n",
    "            \n",
    "            spamtime += time.clock() - endtime\n",
    "#             print spamtime\n",
    "            endtime = time.clock()\n",
    "            \n",
    "    print \"stopped\"\n",
    "\n",
    "for i, meme in enumerate(fresh_memes[1:3]):\n",
    "    print meme\n",
    "    chan = \"#tsm_theoddone\"\n",
    "\n",
    "    filename = chan+\"_\"+str(i)+\"_testspam.txt\"\n",
    "    spam(chan, filename, meme, username, oauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method that lets us spam our generated memes in a particular chat. We now just count the number of times our meme appeared in the following chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it doesnt give EleGiggle: 0\n",
      "jungler ?? !!: 0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "memes= [\"it doesnt give EleGiggle\", \"jungler ?? !!\" ]\n",
    "for meme, fname in zip(memes, glob.glob(\"spam/*\")):\n",
    "    counter = 0\n",
    "    with open(fname, 'r') as f:\n",
    "\n",
    "        for i in f:\n",
    "\n",
    "            if meme in i:\n",
    "                \n",
    "                counter += 1\n",
    "    print meme + \": \" + str(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can clearly see above, the generator did not work. While some of these generated memes looked similar to nonsensical Twitch chat, nobody rechatted the lines, \n",
    "\n",
    "One of the major problems was probably our ngram model. The trigram model we used only stored messages that had 3 tokens or more. While trigrams are usually good for analyzing text, Twitch chat messages tend to be very short or extremely long, which isn't understood by a trigram model. Another problem with the trigram model was that a lot of chat messages frequently consisted of unique tokens such as ascii art or unique emotes represented as proper nouns. Due to the nature of the ngram model, these unique tokens frequently skew the generator because since most of these messages are only sent once, the model only has 1 message to generate from, leading to unoriginal memes.\n",
    "\n",
    "We also hypothesize that memes that naturally occur in twitch chat often occur at specific times, as a relation to events that happen on the stream. We can see this with high clusters of chat data being transmitted in close proximity of each other. Due to our lackluster performance of our ngram model, we cannot confirm that memes generated from twitch chat itself will form and take shape in new memes. This leans toward our idea that memes are in fact generated as a response to content from the streamer rather than the chat itself. Thus twitch chat probably does not create new memes.\n",
    "\n",
    "For further exploration in generating memes, one should look into ways that augment the ngram model, or other, better predictive models. An easy augmentation could be using a backoff model. A backoff model is a ngram model that combines multiple ngram models into 1. It will always try to match on the longest ngram possible, but when that fails, that model will back off and use a ngram with a lower n. This model will capture more of the short length tokens while providing context for longer length tokens. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html \n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "https://github.com/RubbixCube/Twitch-Chat-Bot-V2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "0083dfc7-0e11-49b5-8a69-e0c6795e5a40": {
     "id": "0083dfc7-0e11-49b5-8a69-e0c6795e5a40",
     "prev": "3a4a8dc0-209f-4ea0-82c8-e30d23fcfa97",
     "regions": {
      "1fed244a-fecc-4e68-b98b-e3605b912c24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0467733-3e45-45b3-87ec-3061a376a2dc",
        "part": "whole"
       },
       "id": "1fed244a-fecc-4e68-b98b-e3605b912c24"
      }
     }
    },
    "12344cbf-4cad-419e-8ccd-785ba76f9825": {
     "id": "12344cbf-4cad-419e-8ccd-785ba76f9825",
     "prev": "3f3ba8ad-b02a-4bff-8928-4b85de8f51a4",
     "regions": {
      "5e296c51-9558-4ce8-bf44-c023a615b928": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7eed4446-dd94-4b06-bd61-cab103f3a29c",
        "part": "whole"
       },
       "id": "5e296c51-9558-4ce8-bf44-c023a615b928"
      }
     }
    },
    "2c97be4e-ea22-4ed4-ba35-9850c08207c6": {
     "id": "2c97be4e-ea22-4ed4-ba35-9850c08207c6",
     "prev": "fff6474e-aba5-4c69-9f32-e256c037fe6f",
     "regions": {
      "7f063dc4-db5a-4750-ae09-1d597f099892": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "125062b5-fbf2-4f06-9e58-331f0c0f2b62",
        "part": "whole"
       },
       "id": "7f063dc4-db5a-4750-ae09-1d597f099892"
      }
     }
    },
    "3045d683-3cfb-4d68-bfb7-e3bd786d641b": {
     "id": "3045d683-3cfb-4d68-bfb7-e3bd786d641b",
     "prev": "af5c4cc3-d10c-49a9-9b95-8f1071893fa5",
     "regions": {
      "e6344607-eb2c-409b-a9c1-91a9f6c7da15": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b6f88f72-4ac7-4f7d-a3e2-47cd5a339fcb",
        "part": "whole"
       },
       "id": "e6344607-eb2c-409b-a9c1-91a9f6c7da15"
      }
     }
    },
    "3a4a8dc0-209f-4ea0-82c8-e30d23fcfa97": {
     "id": "3a4a8dc0-209f-4ea0-82c8-e30d23fcfa97",
     "prev": "955ff01b-aa75-4632-afef-f68131eb039f",
     "regions": {
      "78ad0113-6392-4d2f-aba7-0cd010486ff3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "405e9ee2-e544-4530-b3ab-b4cb2d105f4c",
        "part": "whole"
       },
       "id": "78ad0113-6392-4d2f-aba7-0cd010486ff3"
      }
     }
    },
    "3f3ba8ad-b02a-4bff-8928-4b85de8f51a4": {
     "id": "3f3ba8ad-b02a-4bff-8928-4b85de8f51a4",
     "prev": "c8940da2-00f3-4f4e-b253-58ac76b5c9e0",
     "regions": {
      "f09e61be-406b-4113-93d3-c636079629d0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c6825f66-5cc4-443e-93bf-f36d69fb20b9",
        "part": "whole"
       },
       "id": "f09e61be-406b-4113-93d3-c636079629d0"
      }
     }
    },
    "40c656b3-39e2-4774-9045-ec3acf222a82": {
     "id": "40c656b3-39e2-4774-9045-ec3acf222a82",
     "prev": "3045d683-3cfb-4d68-bfb7-e3bd786d641b",
     "regions": {
      "30261b7f-6647-4210-af61-85faf90e3b44": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "200cd011-a795-4736-8c68-03ab562ce129",
        "part": "whole"
       },
       "id": "30261b7f-6647-4210-af61-85faf90e3b44"
      }
     }
    },
    "471136c2-b714-4245-8eef-471ed96660b9": {
     "id": "471136c2-b714-4245-8eef-471ed96660b9",
     "prev": null,
     "regions": {
      "0d57ad74-0f96-4943-a846-0839fe1f5d1a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "27ce94da-0235-4af5-b2f7-5b6509511f3c",
        "part": "whole"
       },
       "id": "0d57ad74-0f96-4943-a846-0839fe1f5d1a"
      }
     }
    },
    "68d5ca3e-b597-40df-aa85-d65743bc3292": {
     "id": "68d5ca3e-b597-40df-aa85-d65743bc3292",
     "prev": "471136c2-b714-4245-8eef-471ed96660b9",
     "regions": {
      "6974b501-a4b8-4058-b890-60befa7627c5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b7d2a692-df38-492c-9193-83ca05bbade4",
        "part": "whole"
       },
       "id": "6974b501-a4b8-4058-b890-60befa7627c5"
      }
     }
    },
    "6fb6abf3-9ee7-4b8d-b1f1-893f67c02ce0": {
     "id": "6fb6abf3-9ee7-4b8d-b1f1-893f67c02ce0",
     "prev": "2c97be4e-ea22-4ed4-ba35-9850c08207c6",
     "regions": {
      "c7d72b83-5f8d-4671-b811-1a6980756902": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "dba1bdb2-8b74-4a79-b451-00636a89d79b",
        "part": "whole"
       },
       "id": "c7d72b83-5f8d-4671-b811-1a6980756902"
      }
     }
    },
    "955ff01b-aa75-4632-afef-f68131eb039f": {
     "id": "955ff01b-aa75-4632-afef-f68131eb039f",
     "prev": "40c656b3-39e2-4774-9045-ec3acf222a82",
     "regions": {
      "1e47af1a-b234-482a-a4c9-30227396389f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "53dbb6ce-cf5c-4527-b7d7-b5f7f46d7148",
        "part": "whole"
       },
       "id": "1e47af1a-b234-482a-a4c9-30227396389f"
      }
     }
    },
    "af5c4cc3-d10c-49a9-9b95-8f1071893fa5": {
     "id": "af5c4cc3-d10c-49a9-9b95-8f1071893fa5",
     "prev": "6fb6abf3-9ee7-4b8d-b1f1-893f67c02ce0",
     "regions": {
      "36e53049-76d8-42f4-a40e-ea9209552301": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e0a2dad3-9403-44eb-9a75-7410a8a5c39e",
        "part": "whole"
       },
       "id": "36e53049-76d8-42f4-a40e-ea9209552301"
      }
     }
    },
    "c8940da2-00f3-4f4e-b253-58ac76b5c9e0": {
     "id": "c8940da2-00f3-4f4e-b253-58ac76b5c9e0",
     "prev": "68d5ca3e-b597-40df-aa85-d65743bc3292",
     "regions": {
      "cf6e16ae-d306-4c69-9c63-3b8be8838bbd": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40be4587-f94e-4dcf-afd2-1080306f0a50",
        "part": "whole"
       },
       "id": "cf6e16ae-d306-4c69-9c63-3b8be8838bbd"
      }
     }
    },
    "fff6474e-aba5-4c69-9f32-e256c037fe6f": {
     "id": "fff6474e-aba5-4c69-9f32-e256c037fe6f",
     "prev": "12344cbf-4cad-419e-8ccd-785ba76f9825",
     "regions": {
      "637e0e54-7264-4fa8-9afa-7f695b15423d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40a49f59-178b-4bd8-adba-2560cc5726c8",
        "part": "whole"
       },
       "id": "637e0e54-7264-4fa8-9afa-7f695b15423d"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
