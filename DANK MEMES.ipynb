{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "27ce94da-0235-4af5-b2f7-5b6509511f3c"
    }
   },
   "source": [
    "The collective ultimate dank meme generator using ngram analysis. Crowd mentality is often found in cases where anonymity and brief exposure reign free in communicative environments. Using twitch chat data we will analyze for patterns of high repetition phrases such as \"LUL\", \"Harambe\", and \"9/11 was an inside job\" as well as other emoticons such as \"Kappa\" and \"PogChamp\". We will call these words \"memes\" and use a statistic \"dankness\" to measure how virulent these words are. Using an N-gram model we will generate subsequent \"memes\" based off the dankness statistic to generate the most likely to be used dank meme.\n",
    "\n",
    "We use the twitch chat API to set up an IRC bot that sits in various channels that records the output of these channels. We decided to use these channels based off popularity; we intend to see a correlation between dankness and popularity of channel. These chat logs are massive in size since anonymity gives people leeway to post whatever they want whether it be offensive, politically charged, or pure nonsense. We note that in general, it is easier to convey a certain message with a shorter amount of words which can often lead to short non-sensical words that are generally accepted by the chat to have a certain meaning. We see that in the data, short phrases are favored over longer sentences. In fact, twitch has its own emoticons usually some sort of facial expression to indicate a message. One exception to the rule are \"copy-pastas\" which are long, pre-written paragraphs of text that are copy and pasted throughout chat, usually indicating satirical undertones although sometimes more direct. Chat will also react to things going on during the stream session. From this we can see clustering of chats being sent (which will be indicative from timestamps) around certain times usually of similar content.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b7d2a692-df38-492c-9193-83ca05bbade4"
    }
   },
   "source": [
    "How does the content and virality of memes affect twitch chat and the advent of new memes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "40be4587-f94e-4dcf-afd2-1080306f0a50"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up for methods and stuff\n",
    "\n",
    "import datetime\n",
    "import socket\n",
    "import select\n",
    "import re\n",
    "\n",
    "channels = [\n",
    "    'tsm_bjergsen',\n",
    "    'c9sneaky',\n",
    "    'trick2g',\n",
    "    'tsm_doublelift',\n",
    "    'nightblue3',\n",
    "    'imaqtpie',\n",
    "    'rush',\n",
    "    'admiralbulldog',\n",
    "    'tsm_theoddone',\n",
    "    'wingsofdeath'\n",
    "    ]\n",
    "username = 'dankmemebot1'\n",
    "oauth = 'oauth:f1d7mm17vlzjols100etso2zg9jqru'\n",
    "\n",
    "channelfiles = {}\n",
    "\n",
    "for name in channels:\n",
    "    channelfiles[name] = open(name+'.txt', 'a+')\n",
    "#     channelfiles[name].write(\"\\n \\n------------ \\n \\n \\n \\n\")\n",
    "#     channelfiles[name].write(\"new session\\n\")\n",
    "#     channelfiles[name].write(\"\\n \\n \\n ------------ \\n \\n \\n\")\n",
    "\n",
    "def ping():\n",
    "    ''' Respond to the server 'pinging' (Stays connected) '''\n",
    "    socks[0].send('PONG :pingis\\n')\n",
    "    print('PONG: Client > tmi.twitch.tv')\n",
    "\n",
    "def sendmsg(chan,msg):\n",
    "    ''' Send specified message to the channel '''\n",
    "    socks[0].send('PRIVMSG '+chan+' :'+msg+'\\n')\n",
    "    print('[BOT] -> '+chan+': '+msg+'\\n')\n",
    "\n",
    "def sendwhis(user,msg):\n",
    "    socks[1].send('PRIVMSG #jtv :/w '+user+' '+msg+'\\n')\n",
    "    print('[BOT] -> '+user+': '+msg+'\\n')\n",
    "\n",
    "def getmsg(msg):\n",
    "    ''' GET IMPORTANT MESSAGE '''\n",
    "    if(re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)):\n",
    "        msg_edit = msg.split(':',2)\n",
    "        if(len(msg_edit) > 2):\n",
    "            user = msg_edit[1].split('!',1)[0] # User\n",
    "            message = msg_edit[2] # Message\n",
    "            channel = re.findall('PRIVMSG (.*)',msg_edit[1]) # Channel\n",
    "            #print channel[0][1:-1]\n",
    "            privmsg = re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)\n",
    "            ''' CONVERT TO ARRAY '''\n",
    "            privmsg = [x for xs in privmsg for x in xs]\n",
    "\n",
    "            datelog = datetime.datetime.now()\n",
    "\n",
    "            ''' PRINT TO CONSOLE '''\n",
    "            if(len(privmsg) > 0):\n",
    "                if len(channel) > 0:\n",
    "                    chan = channel[0] \n",
    "                    if len(chan) >= 3:\n",
    "    \n",
    "                        #print ('['+str(datelog.hour)+':'+str(datelog.minute)+':'+str(datelog.second)+'] '+user+' @ '+channel[0][:-1]+': '+message+'\\n')\n",
    "                        channelfiles[channel[0][1:-1]].write('['+str(datelog.day)+':'+str(datelog.hour)+':'+str(datelog.minute)+':'+str(datelog.second)+'] '+user+' @ '+channel[0][:-1]+': '+message+'\\n')\n",
    "                \n",
    "    if(re.findall('@(.*).tmi.twitch.tv WHISPER (.*) :(.*)',msg)):\n",
    "        whisper = re.findall('@(.*).tmi.twitch.tv WHISPER (.*) :(.*)',msg)\n",
    "        whisper = [x for xs in whisper for x in xs]\n",
    "\n",
    "        ''' PRINT TO CONSOLE '''\n",
    "        if(len(whisper) > 0):\n",
    "            ''' PRINT WHISPER TO CONSOLE '''\n",
    "            print('*WHISPER* '+whisper[0]+': '+whisper[2])\n",
    "            \n",
    "print \"finished\"\n",
    "socks = [socket.socket(),socket.socket()]\n",
    "socks[0].connect(('irc.twitch.tv',6667))\n",
    "\n",
    "socks[0].send('PASS '+oauth+'\\n')\n",
    "socks[0].send('NICK '+username+'\\n')\n",
    "#https://github.com/RubbixCube/Twitch-Chat-Bot-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "socks = [socket.socket(),socket.socket()]\n",
    "\n",
    "socks[0].connect(('irc.twitch.tv',6667))\n",
    "\n",
    "socks[0].send('PASS '+oauth+'\\n')\n",
    "socks[0].send('NICK '+username+'\\n')\n",
    "\n",
    "# socks[0].send(\"JOIN #tsm_bjergsen \\r\\n\")\n",
    "\n",
    "for val in channels:\n",
    "\n",
    "    socks[0].send('JOIN #'+val+'\\n')\n",
    "    \n",
    "print('Connected to irc.twitch.tv on port 6667')\n",
    "print('USER: '+username)\n",
    "print('OAUTH: oauth:'+'*'*len(oauth))\n",
    "print('\\n')\n",
    "\n",
    "temp = 0\n",
    "count = 0\n",
    "while True:\n",
    "  \n",
    "    (sread,swrite,sexc) = select.select(socks,socks,[],120)\n",
    "    for sock in sread:\n",
    "  \n",
    "        ''' Receive data from the server '''\n",
    "        msg = sock.recv(2048)\n",
    "        if(msg == ''):\n",
    "            temp + 1\n",
    "            if(temp > 5):\n",
    "                print('Connection might have been terminated')\n",
    "    \n",
    "        ''' Remove any linebreaks from the message '''\n",
    "        msg = msg.strip('\\n\\r')\n",
    "\n",
    "        ''' DISPLAY MESSAGE IN SHELL '''\n",
    "        getmsg(msg)\n",
    "#         print(msg)\n",
    "\n",
    "\n",
    "        # ANYTHING TO DO WITH CHAT FROM CHANNELS\n",
    "        ''' GET THE INFO FROM THE SERVER '''\n",
    "        check = re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)\n",
    "        if(len(check) > 0):\n",
    "            msg_edit = msg.split(':',2)\n",
    "            if(len(msg_edit) > 2):\n",
    "                user = msg_edit[1].split('!',1)[0] # User\n",
    "                message = msg_edit[2] # Message\n",
    "                channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "\n",
    "                msg_split = str.split(message)\n",
    " \n",
    "                \n",
    "\n",
    "                        \n",
    "        # ANYTHING TO DO WITH WHISPERS RECIEVED FROM USERS\n",
    "        check = re.findall('@(.*).tmi.twitch.tv WHISPER (.*) :(.*)',msg)\n",
    "        if(len(check) > 0):\n",
    "            msg_edit = msg.split(':',2)\n",
    "            if(len(msg) > 2):\n",
    "                user = msg_edit[1].split('!',1)[0] # User\n",
    "                message = msg_edit[2] # Message\n",
    "                channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "\n",
    "                whis_split = str.split(message)\n",
    "                               \n",
    " \n",
    "\n",
    "        ''' Respond to server pings '''\n",
    "        if msg.find('PING :') != -1:\n",
    "            print('PING: tmi.twitch.tv > Client')\n",
    "            ping()\n",
    "print \"stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c6825f66-5cc4-443e-93bf-f36d69fb20b9"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7eed4446-dd94-4b06-bd61-cab103f3a29c"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "channels = [\n",
    "    'tsm_bjergsen',\n",
    "    'c9sneaky',\n",
    "    'trick2g',\n",
    "    'tsm_doublelift',\n",
    "    'nightblue3',\n",
    "    'imaqtpie',\n",
    "    'rush'\n",
    "    ]\n",
    "\n",
    "def parseTwitchChat(textpath):\n",
    "    #returns an array of text\n",
    "    text = []\n",
    "    with open(textpath, 'r') as f:\n",
    "        for line in f:\n",
    "            text.append(line.split())\n",
    "    count = 0\n",
    "    newtext = []\n",
    "    for line in text:\n",
    "        \n",
    "        if len(line) > 2:\n",
    "            count += 1\n",
    "            if line[1] == 'PRIVMSG':\n",
    "                line = line[3:]\n",
    "                \n",
    "                line[0] = line[0][1:]\n",
    "                newtext.append(line)\n",
    "#                 if count <= 10:\n",
    "#                     print line\n",
    "            else:    \n",
    "                line = line[4:]\n",
    "                newtext.append(line)\n",
    "#                 if count <= 10:\n",
    "#                     print line\n",
    "\n",
    "\n",
    "    return newtext\n",
    "\n",
    "textFiles = {}\n",
    "labels = []\n",
    "\n",
    "for i in channels:\n",
    "    textFiles[i] = (parseTwitchChat(i + \".txt\"))\n",
    "    labels.append(i)\n",
    "    \n",
    "for i in channels:\n",
    "   \n",
    "    for k in range(len(textFiles[i])): #for array inside textfile\n",
    "        if (k % 1000) == 0:\n",
    "            with open('league/'+i+'/' + i + '_' + str(k) +'_parsed.txt', 'a+') as f: #open every 1000 lines\n",
    "                f.truncate()\n",
    "                lenleftover = len(textFiles[i]) - k\n",
    "                if lenleftover >= 1000:\n",
    "                    lenleftover = 1000\n",
    "                \n",
    "                for j in range(lenleftover): #indivudal word in array\n",
    "                    for l in textFiles[i][j+k]: #iterate through 1000 times for every 1000 lines\n",
    "                        f.write(l +' ') #l = word\n",
    "                    f.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print \"finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "40a49f59-178b-4bd8-adba-2560cc5726c8"
    }
   },
   "source": [
    "## Naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "125062b5-fbf2-4f06-9e58-331f0c0f2b62"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "#load files from very specific format check \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn-datasets-load-files\n",
    "files_train = load_files('league')\n",
    "\n",
    "\n",
    "#vectorize the data\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "\n",
    "#vector transform \n",
    "# print unicode(files_train.data)\n",
    "X_train_counts = count_vect.fit_transform([unicode(s, errors='replace') for s in files_train.data])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "\n",
    "X_train_tfidf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, files_train.target)\n",
    "\n",
    "files_test = load_files('test')\n",
    "\n",
    "test_vect = CountVectorizer()\n",
    "\n",
    "test_train_counts = count_vect.transform(files_test.data)\n",
    "\n",
    "Test_train_tfidf = tf_transformer.transform(test_train_counts)\n",
    "\n",
    "ypred = clf.predict(Test_train_tfidf)\n",
    "print (files_test.target != ypred).sum(), len(files_test.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dba1bdb2-8b74-4a79-b451-00636a89d79b"
    }
   },
   "source": [
    "How mindless is twitch chat?\n",
    "\n",
    "The average twitch user’s chat logs is made up of two kinds of messages: spam and the creation of messages for other users to spam. Because of the genuine lack of creativity and intelligence among the average twitch user’s chat logs, we hypothesize that a computer can emulate both kinds of messages. \n",
    "\n",
    "Emulating spam is trivial; often in twitch chat one message is spammed so often that it becomes difficult to see messages of any other kind. A computer would have no problem keeping track of the frequencies of messages so its an easy task for a computer.\n",
    "\n",
    "Original messages are much more difficult to create. First, in order to try and emulate creative messages, we will establish a measure called dankness. We believe that a successful original message will be spammed more than unsuccessful ones, so dankness will be defined as the number of times a phrase was chatted. Due to the seemingly randomness nature of Twitch chat, we first approach this problem using an ngram model run on the Twitch chat data. We tried using a trigram model at first, but we found that a interpolated model with bigram and unigrams works a lot better due to the small average length of each message. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Meme Generation\n",
    "\n",
    "In order to analyze how the content of memes affect future memes, we need a way to use old memes to create new memes and determine if these new memes are viral. An ngram language model is a simple language model that fits this exact need. The ngram language model trains on past data and extracts the ngrams, word tuples of length n, from the text. This model captures the relationship between words that frequently occur together, which it uses to generate phrases. \n",
    "\n",
    "To make a ngram model, we need to first process the data. We added !END! tags to the end of every chat line to indicate the end of a message. We then tagged all tokens with less than 5 occurances in the file as UNKNOWN words, which we will deal with later. We create our ngram model with all of our collected data in order to create the most wholesome memes. To generate a phrase, we start with a random ngram and look at all words that followed it in our data. We then randomly choose a word from that list with the most occuring word more likely to be chosen. We keep generating until the maximum length has been reached or an !END! tag has been reached. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b6f88f72-4ac7-4f7d-a3e2-47cd5a339fcb"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import nltk\n",
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "from itertools import groupby as g\n",
    "\n",
    "def tokenize_by_word(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def most_common(L):\n",
    "    return max(g(sorted(L)), key=lambda(x, v):(len(list(v)),-L.index(x)))[0]\n",
    "class NgramModel:\n",
    "    \"\"\"\n",
    "\n",
    "    __init__ initializes a NgramModel. \n",
    "    training preprocessing: add !START! and !END! tokens to each line. \n",
    "    Parameters:\n",
    "    n                       the n used in the ngram model\n",
    "    train                   a string containing a training text\n",
    "    unknown_replace_limit   if a word in the training text appears less than this many times, replace it with 'UNKNOWNWORD'\n",
    "    \"\"\"\n",
    "    def __init__(self, n,tokens=[], train=\"\",filenames=\"\", UnknownCount=0):\n",
    "        self.filenames = filenames\n",
    "        self.UnknownCount = UnknownCount\n",
    "        if (n < 0):\n",
    "            raise Exception(\"N must be greater than or equal to zero to make an Ngram model.\")\n",
    "        if len(tokens) == 0:\n",
    "            self.tokens = self.tokenize_from_files()\n",
    "            self.tag_with_unknown()\n",
    "        else:\n",
    "            self.tokens = tokens\n",
    "            self.tag_with_unknown()\n",
    "\n",
    "        self.n = n\n",
    "        \n",
    "\n",
    "        self.model = self.build_model()\n",
    "#         self.ngrams_count = ngrams_count\n",
    "#         self.n1grams_count = n1grams_count\n",
    "#         self.vocab_size = len(set(tokens))\n",
    "    def tag_with_unknown(self):\n",
    "        token_count = Counter(self.tokens)\n",
    "        def mark(token):\n",
    "            if token_count[token] < self.UnknownCount:\n",
    "                return \"!UNKNOWN!\"\n",
    "            else:\n",
    "                return token\n",
    "        tokens = [mark(token) for token in self.tokens ]\n",
    "        self.tokens = tokens\n",
    "    def tokenize_from_files(self):\n",
    "        file_tokens = []\n",
    "        for fname in glob.glob(self.filenames):\n",
    "            with open(fname, 'r') as f:\n",
    "\n",
    "                for i in f:\n",
    "                    tokens = i.split()\n",
    "\n",
    "\n",
    "                    tokens.append(None)\n",
    "\n",
    "\n",
    "                    file_tokens.extend(tokens)\n",
    "        return file_tokens\n",
    "    def build_model(self):\n",
    "    #     build a language model from ngrams\n",
    "        tokens = self.tokens\n",
    "        n = self.n\n",
    "        model = dict()\n",
    "        if len(tokens) < n:\n",
    "            return model\n",
    "        for i in range(len(tokens) - n):\n",
    "            gram = tuple(tokens[i:i+n])\n",
    "            next_tok = tokens[i+n]\n",
    "            if gram in model:\n",
    "                model[gram].append(next_tok)\n",
    "            else: \n",
    "                model[gram] = [next_tok]\n",
    "        final_gram = tuple(tokens[len(tokens)-n:])\n",
    "#         if final_gram in model:\n",
    "#             model[final_gram].append(None)\n",
    "#         else:\n",
    "#             model[final_gram] = [None]\n",
    "        return model\n",
    "    def generate(self, seed=None, max_iterations=50):\n",
    "        model = self.model\n",
    "#         print model\n",
    "        n = self.n\n",
    "        seed = random.choice(model.keys())\n",
    "        flag = True\n",
    "        while flag:\n",
    "            flag = False\n",
    "            for i in seed:\n",
    "                if i is None or i == \"!UNKNOWN!\":\n",
    "                    flag = True\n",
    "            if flag:\n",
    "                seed = random.choice(model.keys())\n",
    "\n",
    "        output = list(seed)\n",
    "        current = tuple(seed)\n",
    "        for i in range(max_iterations):\n",
    "            next_token = random.choice(model.keys())[0]\n",
    "            while next_token == \"!UNKNOWN!\":\n",
    "                next_token = random.choice(model.keys())[0]\n",
    "            if current in model:\n",
    "                possible_next_tokens = model[current]\n",
    "                if len(possible_next_tokens) > 1:\n",
    "                    next_token = random.choice(possible_next_tokens)\n",
    "                    while next_token == \"!UNKNOWN!\":\n",
    "                        possible_next_tokens.remove(next_token)\n",
    "                        try:\n",
    "                            next_token = random.choice(possible_next_tokens)\n",
    "                        except:\n",
    "                            next_token = random.choice(model.keys())[0]\n",
    "                            while next_token == \"!UNKNOWN!\":\n",
    "                                next_token = random.choice(model.keys())[0]\n",
    "\n",
    "            if next_token is None: break\n",
    "#             print next_token == \"!UNKNOWN!\", next_token\n",
    "            \n",
    "            output.append(next_token)\n",
    "            current = tuple(output[-n:])\n",
    "        return output\n",
    "\n",
    "\n",
    "files = []\n",
    "file_tokens = []\n",
    "for fname in glob.glob(\"league/*/*.txt\"):\n",
    "    with open(fname, 'r') as f:\n",
    "        \n",
    "        for i in f:\n",
    "            tokens = i.split()\n",
    "\n",
    "\n",
    "            if len(tokens) > 0 and \"@\" not in tokens[0] and \"!\" not in tokens[0] :\n",
    "\n",
    "                tokens.append(None)\n",
    "                file_tokens.extend(tokens)\n",
    "\n",
    "    break\n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "# print file_tokens[0:100]\n",
    "bigram = NgramModel(3, file_tokens, filenames=\"league/*/*.txt\", UnknownCount=15)\n",
    "# bigram_unfiltered = NgramModel(2, filenames=\"league/*/*.txt\")\n",
    "\n",
    "print \"model created\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "200cd011-a795-4736-8c68-03ab562ce129"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoHiYo PogChamp Kreygasm\n",
      "\n",
      "qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE .\n",
      "\n",
      "sneakyWeeb ME WEEB HotPokket LOL IT\n",
      "\n",
      "LOL LOL LOL sneakyFedora\n",
      "\n",
      "bunWeeb Weeb bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb bunWeeb Me Weeb bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb\n",
      "\n",
      "SourPls qtpDANCE SneakyPls qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE\n",
      "\n",
      "bUrself the bUrself IT TriHard\n",
      "\n",
      "SneakyPls qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE qtpPLS SourPls qtpDANCE .\n",
      "\n",
      "bunWeeb Weeb bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb .\n",
      "\n",
      "Weeb bunWeeb Me Spam bunWeeb Mods Baka bunWeeb If Ban bunWeeb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fresh_memes = []\n",
    "for i in range(10):\n",
    "    meme = \" \".join(bigram.generate())\n",
    "    print meme\n",
    "    print \n",
    "    fresh_memes.append(meme)\n",
    "\n",
    "# print fresh_memes\n",
    "# for i in range(5):\n",
    "#     print \" \".join(bigram_unfiltered.generate())\n",
    "#     print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "53dbb6ce-cf5c-4527-b7d7-b5f7f46d7148"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it doesnt give EleGiggle\n",
      "Connected to irc.twitch.tv on port 6667\n",
      "USER: dankmemebot1\n",
      "OAUTH: oauth:******************************\n",
      "\n",
      "\n",
      "spam#tsm_theoddone_0_testspam.txt\n",
      "spam#tsm_theoddone_0_testspam.txt\n",
      "spam#tsm_theoddone_0_testspam.txt\n",
      "spam#tsm_theoddone_0_testspam.txt\n",
      "PING: tmi.twitch.tv > Client\n",
      "PONG: Client > tmi.twitch.tv\n",
      "spam#tsm_theoddone_0_testspam.txt\n",
      "spam#tsm_theoddone_0_testspam.txt\n",
      "stopped\n",
      "jungler ?? !!\n",
      "Connected to irc.twitch.tv on port 6667\n",
      "USER: dankmemebot1\n",
      "OAUTH: oauth:******************************\n",
      "\n",
      "\n",
      "spam#tsm_theoddone_1_testspam.txt\n",
      "spam#tsm_theoddone_1_testspam.txt\n",
      "spam#tsm_theoddone_1_testspam.txt\n",
      "spam#tsm_theoddone_1_testspam.txt\n",
      "spam#tsm_theoddone_1_testspam.txt\n",
      "spam#tsm_theoddone_1_testspam.txt\n",
      "stopped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def spam(chan, fname, message1, username, oauth):\n",
    "#     prevchannels = channels\n",
    "#     channels = [chan[1:]]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # socks[0].send(\"JOIN #tsm_bjergsen \\r\\n\")\n",
    "\n",
    "\n",
    "    socks[0].send('JOIN '+chan+'\\n')\n",
    "        \n",
    "    print('Connected to irc.twitch.tv on port 6667')\n",
    "    print('USER: '+username)\n",
    "    print('OAUTH: oauth:'+'*'*30)\n",
    "    print('\\n')\n",
    "\n",
    "    temp = 0\n",
    "    count = 0\n",
    "    starttime = time.clock()\n",
    "    endtime = starttime\n",
    "    spamtime = -1\n",
    "    while (endtime-starttime) <= 180:\n",
    "      \n",
    "        (sread,swrite,sexc) = select.select(socks,socks,[],120)\n",
    "        for sock in sread:\n",
    "              \n",
    "\n",
    "            if spamtime == -1 or spamtime >= 30:\n",
    "                socks[0].send('PRIVMSG '+chan+' :'+message1+'\\n')\n",
    "\n",
    "\n",
    "                print \"spam\"+fname\n",
    "                spamtime = 0\n",
    "                \n",
    "                \n",
    "            ''' Receive data from the server '''\n",
    "            msg = sock.recv(2048)\n",
    "            if(msg == ''):\n",
    "                temp + 1\n",
    "                if(temp > 5):\n",
    "                    print('Connection might have been terminated')\n",
    "        \n",
    "            ''' Remove any linebreaks from the message '''\n",
    "            msg = msg.strip('\\n\\r')\n",
    "\n",
    "            ''' DISPLAY MESSAGE IN SHELL '''\n",
    "            getmsg(msg)\n",
    "            with open(fname, 'a+') as f:\n",
    "                f.write(msg + ' \\n')\n",
    "                f.close()\n",
    "    #         print(msg)\n",
    "\n",
    "\n",
    "            # ANYTHING TO DO WITH CHAT FROM CHANNELS\n",
    "            ''' GET THE INFO FROM THE SERVER '''\n",
    "            check = re.findall('@(.*).tmi.twitch.tv PRIVMSG (.*) :(.*)',msg)\n",
    "            if(len(check) > 0):\n",
    "                msg_edit = msg.split(':',2)\n",
    "                if(len(msg_edit) > 2):\n",
    "                    user = msg_edit[1].split('!',1)[0] # User\n",
    "                    message = msg_edit[2] # Message\n",
    "                    channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "\n",
    "                    msg_split = str.split(message)\n",
    "     \n",
    "                    \n",
    "\n",
    "                            \n",
    "            # ANYTHING TO DO WITH WHISPERS RECIEVED FROM USERS\n",
    "            check = re.findall('@(.*).tmi.twitch.tv WHISPER (.*) :(.*)',msg)\n",
    "            if(len(check) > 0):\n",
    "                msg_edit = msg.split(':',2)\n",
    "                if(len(msg) > 2):\n",
    "                    user = msg_edit[1].split('!',1)[0] # User\n",
    "                    message = msg_edit[2] # Message\n",
    "                    channel = msg_edit[1].split(' ',2)[2][:-1] # Channel\n",
    "\n",
    "                    whis_split = str.split(message)\n",
    "                                   \n",
    "     \n",
    "\n",
    "            ''' Respond to server pings '''\n",
    "            if msg.find('PING :') != -1:\n",
    "                print('PING: tmi.twitch.tv > Client')\n",
    "                ping()\n",
    "            \n",
    "            spamtime += time.clock() - endtime\n",
    "#             print spamtime\n",
    "            endtime = time.clock()\n",
    "            \n",
    "    print \"stopped\"\n",
    "#     textFiles = {}\n",
    "#     labels = []\n",
    "\n",
    "#     for i in channels:\n",
    "#         textFiles[i] = (parseTwitchChat(i + \".txt\"))\n",
    "#         labels.append(i)\n",
    "        \n",
    "#     for i in channels:\n",
    "       \n",
    "#         for k in range(len(textFiles[i])): #for array inside textfile\n",
    "#             if (k % 1000) == 0:\n",
    "#                 with open('league/' + i + '_' + str(k) +'_parsedresponse.txt', 'a+') as f: #open every 1000 lines\n",
    "#                     f.truncate()\n",
    "#                     lenleftover = len(textFiles[i]) - k\n",
    "#                     if lenleftover >= 1000:\n",
    "#                         lenleftover = 1000\n",
    "                    \n",
    "#                     for j in range(lenleftover): #indivudal word in array\n",
    "#                         for l in textFiles[i][j+k]: #iterate through 1000 times for every 1000 lines\n",
    "#                             f.write(l +' ') #l = word\n",
    "#                         f.write('\\n')\n",
    "    \n",
    "    \n",
    "#     channels = prevchannels\n",
    "# for i, meme in enumerate(fresh_memes[1:2]):\n",
    "#     print meme\n",
    "#     chan = \"#wingsofdeath\"\n",
    "\n",
    "#     filename = chan+\"_\"+str(i+1)+\"_testspam.txt\"\n",
    "#     spam(chan, filename, meme, username, oauth)\n",
    "for i, meme in enumerate(fresh_memes[1:3]):\n",
    "    print meme\n",
    "    chan = \"#tsm_theoddone\"\n",
    "\n",
    "    filename = chan+\"_\"+str(i)+\"_testspam.txt\"\n",
    "    spam(chan, filename, meme, username, oauth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it doesnt give EleGiggle: 0\n",
      "jungler ?? !!: 0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "memes= [\"it doesnt give EleGiggle\", \"jungler ?? !!\" ]\n",
    "for meme, fname in zip(memes, glob.glob(\"spam/*\")):\n",
    "    counter = 0\n",
    "    with open(fname, 'r') as f:\n",
    "\n",
    "        for i in f:\n",
    "\n",
    "            if meme in i:\n",
    "                \n",
    "                counter += 1\n",
    "    print meme + \": \" + str(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Memes Conclusion\n",
    "\n",
    "As we can clearly see above, the generator did not work. While some of these generated memes looked similar to nonsensical Twitch chat, nobody rechatted the lines, \n",
    "\n",
    "In addition, our ngram model is also not without fault. The trigram model we used only stored messages that had 3 tokens or more. While trigrams are usually good for analyzing text, Twitch chat messages tend to be very short or extremely long, which isn't understood by a trigram model. Another problem with the trigram model was that a lot of chat messages frequently consisted of unique tokens such as ascii art or unique emotes represented as proper nouns. Due to the nature of the ngram model, these unique tokens frequently skew the generator because since most of these messages are only sent once, the model only has 1 message to generate from, leading to unoriginal memes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further Exploration\n",
    "## Meme Generation\n",
    "\n",
    "For further exploration in generating memes, one should look into ways that augment the ngram model, or other, better predictive models. An easy augmentation could be using a backoff model. A backoff model is a ngram model that combines multiple ngram models into 1. It will always try to match on the longest ngram possible, but when that fails, that model will back off and use a ngram with a lower n. This model will capture more of the short length tokens while providing context for longer length tokens. A better predictive model would be doing unsupervised training over the chat to find any underlying relationships and then creating a Markov model that generates a phrase based on the relationships that were found. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "405e9ee2-e544-4530-b3ab-b4cb2d105f4c"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a0467733-3e45-45b3-87ec-3061a376a2dc"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "0083dfc7-0e11-49b5-8a69-e0c6795e5a40": {
     "id": "0083dfc7-0e11-49b5-8a69-e0c6795e5a40",
     "prev": "3a4a8dc0-209f-4ea0-82c8-e30d23fcfa97",
     "regions": {
      "1fed244a-fecc-4e68-b98b-e3605b912c24": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0467733-3e45-45b3-87ec-3061a376a2dc",
        "part": "whole"
       },
       "id": "1fed244a-fecc-4e68-b98b-e3605b912c24"
      }
     }
    },
    "12344cbf-4cad-419e-8ccd-785ba76f9825": {
     "id": "12344cbf-4cad-419e-8ccd-785ba76f9825",
     "prev": "3f3ba8ad-b02a-4bff-8928-4b85de8f51a4",
     "regions": {
      "5e296c51-9558-4ce8-bf44-c023a615b928": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7eed4446-dd94-4b06-bd61-cab103f3a29c",
        "part": "whole"
       },
       "id": "5e296c51-9558-4ce8-bf44-c023a615b928"
      }
     }
    },
    "2c97be4e-ea22-4ed4-ba35-9850c08207c6": {
     "id": "2c97be4e-ea22-4ed4-ba35-9850c08207c6",
     "prev": "fff6474e-aba5-4c69-9f32-e256c037fe6f",
     "regions": {
      "7f063dc4-db5a-4750-ae09-1d597f099892": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "125062b5-fbf2-4f06-9e58-331f0c0f2b62",
        "part": "whole"
       },
       "id": "7f063dc4-db5a-4750-ae09-1d597f099892"
      }
     }
    },
    "3045d683-3cfb-4d68-bfb7-e3bd786d641b": {
     "id": "3045d683-3cfb-4d68-bfb7-e3bd786d641b",
     "prev": "af5c4cc3-d10c-49a9-9b95-8f1071893fa5",
     "regions": {
      "e6344607-eb2c-409b-a9c1-91a9f6c7da15": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b6f88f72-4ac7-4f7d-a3e2-47cd5a339fcb",
        "part": "whole"
       },
       "id": "e6344607-eb2c-409b-a9c1-91a9f6c7da15"
      }
     }
    },
    "3a4a8dc0-209f-4ea0-82c8-e30d23fcfa97": {
     "id": "3a4a8dc0-209f-4ea0-82c8-e30d23fcfa97",
     "prev": "955ff01b-aa75-4632-afef-f68131eb039f",
     "regions": {
      "78ad0113-6392-4d2f-aba7-0cd010486ff3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "405e9ee2-e544-4530-b3ab-b4cb2d105f4c",
        "part": "whole"
       },
       "id": "78ad0113-6392-4d2f-aba7-0cd010486ff3"
      }
     }
    },
    "3f3ba8ad-b02a-4bff-8928-4b85de8f51a4": {
     "id": "3f3ba8ad-b02a-4bff-8928-4b85de8f51a4",
     "prev": "c8940da2-00f3-4f4e-b253-58ac76b5c9e0",
     "regions": {
      "f09e61be-406b-4113-93d3-c636079629d0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c6825f66-5cc4-443e-93bf-f36d69fb20b9",
        "part": "whole"
       },
       "id": "f09e61be-406b-4113-93d3-c636079629d0"
      }
     }
    },
    "40c656b3-39e2-4774-9045-ec3acf222a82": {
     "id": "40c656b3-39e2-4774-9045-ec3acf222a82",
     "prev": "3045d683-3cfb-4d68-bfb7-e3bd786d641b",
     "regions": {
      "30261b7f-6647-4210-af61-85faf90e3b44": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "200cd011-a795-4736-8c68-03ab562ce129",
        "part": "whole"
       },
       "id": "30261b7f-6647-4210-af61-85faf90e3b44"
      }
     }
    },
    "471136c2-b714-4245-8eef-471ed96660b9": {
     "id": "471136c2-b714-4245-8eef-471ed96660b9",
     "prev": null,
     "regions": {
      "0d57ad74-0f96-4943-a846-0839fe1f5d1a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "27ce94da-0235-4af5-b2f7-5b6509511f3c",
        "part": "whole"
       },
       "id": "0d57ad74-0f96-4943-a846-0839fe1f5d1a"
      }
     }
    },
    "68d5ca3e-b597-40df-aa85-d65743bc3292": {
     "id": "68d5ca3e-b597-40df-aa85-d65743bc3292",
     "prev": "471136c2-b714-4245-8eef-471ed96660b9",
     "regions": {
      "6974b501-a4b8-4058-b890-60befa7627c5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b7d2a692-df38-492c-9193-83ca05bbade4",
        "part": "whole"
       },
       "id": "6974b501-a4b8-4058-b890-60befa7627c5"
      }
     }
    },
    "6fb6abf3-9ee7-4b8d-b1f1-893f67c02ce0": {
     "id": "6fb6abf3-9ee7-4b8d-b1f1-893f67c02ce0",
     "prev": "2c97be4e-ea22-4ed4-ba35-9850c08207c6",
     "regions": {
      "c7d72b83-5f8d-4671-b811-1a6980756902": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "dba1bdb2-8b74-4a79-b451-00636a89d79b",
        "part": "whole"
       },
       "id": "c7d72b83-5f8d-4671-b811-1a6980756902"
      }
     }
    },
    "955ff01b-aa75-4632-afef-f68131eb039f": {
     "id": "955ff01b-aa75-4632-afef-f68131eb039f",
     "prev": "40c656b3-39e2-4774-9045-ec3acf222a82",
     "regions": {
      "1e47af1a-b234-482a-a4c9-30227396389f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "53dbb6ce-cf5c-4527-b7d7-b5f7f46d7148",
        "part": "whole"
       },
       "id": "1e47af1a-b234-482a-a4c9-30227396389f"
      }
     }
    },
    "af5c4cc3-d10c-49a9-9b95-8f1071893fa5": {
     "id": "af5c4cc3-d10c-49a9-9b95-8f1071893fa5",
     "prev": "6fb6abf3-9ee7-4b8d-b1f1-893f67c02ce0",
     "regions": {
      "36e53049-76d8-42f4-a40e-ea9209552301": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e0a2dad3-9403-44eb-9a75-7410a8a5c39e",
        "part": "whole"
       },
       "id": "36e53049-76d8-42f4-a40e-ea9209552301"
      }
     }
    },
    "c8940da2-00f3-4f4e-b253-58ac76b5c9e0": {
     "id": "c8940da2-00f3-4f4e-b253-58ac76b5c9e0",
     "prev": "68d5ca3e-b597-40df-aa85-d65743bc3292",
     "regions": {
      "cf6e16ae-d306-4c69-9c63-3b8be8838bbd": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40be4587-f94e-4dcf-afd2-1080306f0a50",
        "part": "whole"
       },
       "id": "cf6e16ae-d306-4c69-9c63-3b8be8838bbd"
      }
     }
    },
    "fff6474e-aba5-4c69-9f32-e256c037fe6f": {
     "id": "fff6474e-aba5-4c69-9f32-e256c037fe6f",
     "prev": "12344cbf-4cad-419e-8ccd-785ba76f9825",
     "regions": {
      "637e0e54-7264-4fa8-9afa-7f695b15423d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "40a49f59-178b-4bd8-adba-2560cc5726c8",
        "part": "whole"
       },
       "id": "637e0e54-7264-4fa8-9afa-7f695b15423d"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
